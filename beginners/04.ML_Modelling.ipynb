{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sreddx/redbull-analytics-hol/blob/main/beginners/04.ML_Modelling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyKbW4xDo6M5"
      },
      "source": [
        "# Test For The Best Machine Learning Algorithm For Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J39IynrZo6M8"
      },
      "source": [
        "This notebook takes about 40 minutes to run, but we've already run it and saved the data for you. Please read through it, though, so that you understand how we came to the conclusions we'll use moving forward.\n",
        "\n",
        "## Six Algorithms\n",
        "\n",
        "We're going to compare six different algorithms to determine the best one to produce an accurate model for our predictions.\n",
        "\n",
        "### Logistic Regression\n",
        "\n",
        "Logistic Regression (LR) is a technique borrowed from the field of statistics. It is the go-to method for binary classification problems (problems with two class values). \n",
        "\n",
        "![](https://github.com/Sreddx/redbull-analytics-hol/blob/main/beginners/docs/logisticfunction.png?raw=1)\n",
        "\n",
        "Logistic Regression is named for the function used at the core of the method: the logistic function. The logistic function is a probablistic method used to determine whether or not the driver will be the winner. Logistic Regression predicts probabilities.\n",
        "\n",
        "### Decision Tree\n",
        "\n",
        "A tree has many analogies in real life, and it turns out that it has influenced a wide area of machine learning, covering both classification and regression. In decision analysis, a decision tree can be used to visually and explicitly represent decisions and decision making.\n",
        "\n",
        "![](https://github.com/Sreddx/redbull-analytics-hol/blob/main/beginners/docs/decisiontree.png?raw=1)\n",
        "\n",
        "This methodology is more commonly known as a \"learning decision tree\" from data, and the above tree is called a Classification tree because the goal is to classify a driver as the winner or not.\n",
        "\n",
        "### Random Forest\n",
        "\n",
        "Random forest is a supervised learning algorithm. The \"forest\" it builds is an **ensemble of decision trees**, usually trained with the “bagging” method, a combination of learning models which increases the accuracy of the result.\n",
        "\n",
        "A random forest eradicates the limitations of a decision tree algorithm. It reduces the overfitting of datasets and increases precision. It generates predictions without requiring many configurations.\n",
        "\n",
        "![](https://github.com/Sreddx/redbull-analytics-hol/blob/main/beginners/docs/randomforest.png?raw=1)\n",
        "\n",
        "Here's the difference between the Decision Tree and Random Forest methods:\n",
        "\n",
        "![](https://github.com/Sreddx/redbull-analytics-hol/blob/main/beginners/docs/treefortheforest.jpg?raw=1)\n",
        "\n",
        "### Support Vector Machine Algorithm (SVC)\n",
        "\n",
        "Support Vector Machines (SVMs) are a set of supervised learning methods used for classification, regression and detection of outliers.\n",
        "\n",
        "The advantages of support vector machines are:\n",
        "\n",
        "- Effective in high dimensional spaces\n",
        "- Still effective in cases where number of dimensions is greater than the number of samples\n",
        "- Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient\n",
        "- Versatile: different kernel functions can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels\n",
        "\n",
        "The objective of a SVC (Support Vector Classifier) is to fit to the data you provide, returning a \"best fit\" hyperplane that divides, or categorizes, your data.\n",
        "\n",
        "### Gaussian Naive Bayes Algorithm\n",
        "\n",
        "Naive Bayes is a classification algorithm for binary (two-class) and multi-class classification problems. The technique is easiest to understand when described using binary or categorical input values. The representation used for naive Bayes is probabilities.\n",
        "\n",
        "A list of probabilities is stored to a file for a learned Naive Bayes model. This includes:\n",
        "\n",
        "- **Class Probabilities:** The probabilities of each class in the training dataset.\n",
        "- **Conditional Probabilities:** The conditional probabilities of each input value given each class value.\n",
        "\n",
        "Naive Bayes can be extended to real-value attributes, most commonly by assuming a Gaussian distribution. This extension of Naive Bayes is called Gaussian Naive Bayes. Other functions can be used to estimate the distribution of the data, but the Gaussian (or normal distribution) is the easiest to work with because you only need to estimate the mean and the standard deviation from your training data.\n",
        "\n",
        "### k Nearest Neighbor Algorithm (kNN)\n",
        "\n",
        "The k-Nearest Neighbors (KNN) algorithm is a simple, supervised machine learning algorithm that can be used to solve both classification and regression problems.\n",
        "\n",
        "kNN works by finding the distances between a query and all of the examples in the data, selecting the specified number examples (k) closest to the query, then voting for the most frequent label (in the case of classification) or averages the labels (in the case of regression).\n",
        "\n",
        "The kNN algorithm assumes the similarity between the new case/data and available cases, and puts the new case into the category that is most similar to the available categories.\n",
        "\n",
        "### Gradient boosting classifier (GBC)\n",
        "\n",
        "\n",
        "![](https://github.com/Sreddx/redbull-analytics-hol/blob/main/beginners/docs/knn.png?raw=1)\n",
        "\n",
        "## Analyzing the Data\n",
        "\n",
        "### Feature Importance\n",
        "\n",
        "Another great quality of the random forest algorithm is that it's easy to measure the relative importance of each feature to the prediction.\n",
        "\n",
        "The Scikit-learn Python Library provides a great tool for this which measures a feature's importance by looking at how much the tree nodes that use that feature reduce impurity across all trees in the forest. It computes this score automatically for each feature after training, and scales the results so the sum of all importance is equal to one.\n",
        "\n",
        "### Data Visualization When Building a Model\n",
        "\n",
        "How do you visualize the influence of the data? How do you frame the problem?\n",
        "\n",
        "An important tool in the data scientist's toolkit is the power to visualize data using several excellent libraries such as Seaborn or MatPlotLib. Representing your data visually might allow you to uncover hidden correlations that you can leverage. Your visualizations might also help you to uncover bias or unbalanced data.\n",
        "\n",
        "![](https://github.com/Sreddx/redbull-analytics-hol/blob/main/beginners/docs/visualization.png?raw=1)\n",
        "\n",
        "### Splitting the Dataset\n",
        "\n",
        "Prior to training, you need to split your dataset into two or more parts of unequal size that still represent the data well. \n",
        "\n",
        "1. Training. This part of the dataset is fit to your model to train it. This set constitutes the majority of the original dataset.\n",
        "2. Testing. A test dataset is an independent group of data, often a subset of the original data, that you use to confirm the performance of the model you built.\n",
        "3. Validating. A validation set is a smaller independent group of examples that you use to tune the model's hyperparameters, or architecture, to improve the model. Depending on your data's size and the question you are asking, you might not need to build this third set.\n",
        "\n",
        "## Building the Model\n",
        "\n",
        "Using your training data, your goal is to build a model, or a statistical representation of your data, using various algorithms to train it. Training a model exposes it to data and allows it to make assumptions about perceived patterns it discovers, validates, and accepts or rejects.\n",
        "\n",
        "\n",
        "### Decide on a Training Method\n",
        "\n",
        "Depending on your question and the nature of your data, you will choose a method to train it. Stepping through Scikit-learn's documentation, you can explore many ways to train a model. Depending on the results you get, you might have to try several different methods to build the best model. You are likely to go through a process whereby data scientists evaluate the performance of a model by feeding it unseen data, checking for accuracy, bias, and other quality-degrading issues, and selecting the most appropriate training method for the task at hand.\n",
        "\n",
        "### Train a Model\n",
        "\n",
        "Armed with your training data, you are ready to \"fit\" it to create a model. In many ML libraries you will find the code 'model.fit' - it is at this time that you send in your data as an array of values (usually 'X') and a feature variable (usually 'y').\n",
        "\n",
        "### Evaluate the Model\n",
        "\n",
        "Once the training process is complete, you will be able to evaluate the model's quality by using test data to gauge its performance. This data is a subset of the original data that the model has not previously analyzed. You can print out a table of metrics about your model's quality.\n",
        "\n",
        "#### Model Fitting\n",
        "\n",
        "In the Machine Learning context, model fitting refers to the accuracy of the model's underlying function as it attempts to analyze data with which it is not familiar.\n",
        "\n",
        "#### Underfitting and Overfitting\n",
        "\n",
        "Underfitting and overfitting are common problems that degrade the quality of the model, as the model either doesn't fit well enough, or it fits too well. This causes the model to make predictions either too closely aligned or too loosely aligned with its training data. An overfit model predicts training data too well because it has learned the data's details and noise too well. An underfit model is not accurate as it can neither accurately analyze its training data nor data it has not yet 'seen'.\n",
        "\n",
        "![](https://github.com/Sreddx/redbull-analytics-hol/blob/main/beginners/docs/overfit.png?raw=1)\n",
        "\n",
        "Let's test out some algorithms to choose our path for modelling our predictions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hgEgMSSro6M_"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIZfTH_ho6NB"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "start = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-03T17:56:46.083156Z",
          "start_time": "2020-06-03T17:56:45.184584Z"
        },
        "id": "Z2GAKNPpo6NC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pickle\n",
        "from sklearn.metrics import confusion_matrix, precision_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler,LabelEncoder,OneHotEncoder\n",
        "from sklearn.model_selection import cross_val_score,StratifiedKFold,RandomizedSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import confusion_matrix,precision_score,f1_score,recall_score\n",
        "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
        "plt.style.use('seaborn')\n",
        "\n",
        "np.set_printoptions(precision=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-03T17:56:46.866967Z",
          "start_time": "2020-06-03T17:56:46.697643Z"
        },
        "id": "5ulUHN5Po6NE"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('./data_f1/data_filtered.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-03T17:56:47.463809Z",
          "start_time": "2020-06-03T17:56:47.426867Z"
        },
        "id": "0bE3tXDto6NE"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYCUHg4ro6NF"
      },
      "outputs": [],
      "source": [
        "len(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-jbE635o6NH"
      },
      "outputs": [],
      "source": [
        "dnf_by_driver = data.groupby('driver').sum()['driver_dnf']\n",
        "driver_race_entered = data.groupby('driver').count()['driver_dnf']\n",
        "driver_dnf_ratio = (dnf_by_driver/driver_race_entered)\n",
        "driver_confidence = 1-driver_dnf_ratio\n",
        "driver_confidence_dict = dict(zip(driver_confidence.index,driver_confidence))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fkyw6UWko6NJ"
      },
      "outputs": [],
      "source": [
        "driver_confidence_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yMMMOvg-o6NK"
      },
      "outputs": [],
      "source": [
        "dnf_by_constructor = data.groupby('constructor').sum()['constructor_dnf']\n",
        "constructor_race_entered = data.groupby('constructor').count()['constructor_dnf']\n",
        "constructor_dnf_ratio = (dnf_by_constructor/constructor_race_entered)\n",
        "constructor_reliability = 1-constructor_dnf_ratio\n",
        "constructor_reliability_dict = dict(zip(constructor_reliability.index,constructor_reliability))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYeMqsxGo6NM"
      },
      "outputs": [],
      "source": [
        "constructor_reliability_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L0FH4kxNo6NN"
      },
      "outputs": [],
      "source": [
        "data['driver_confidence'] = data['driver'].apply(lambda x:driver_confidence_dict[x])\n",
        "data['constructor_reliability'] = data['constructor'].apply(lambda x:constructor_reliability_dict[x])\n",
        "#removing retired drivers and constructors\n",
        "active_constructors = ['Alpine F1', 'Williams', 'McLaren', 'Ferrari', 'Mercedes',\n",
        "                       'AlphaTauri', 'Aston Martin', 'Alfa Romeo', 'Red Bull',\n",
        "                       'Haas F1 Team']\n",
        "active_drivers = ['Daniel Ricciardo', 'Mick Schumacher', 'Carlos Sainz',\n",
        "                  'Valtteri Bottas', 'Lance Stroll', 'George Russell',\n",
        "                  'Lando Norris', 'Sebastian Vettel', 'Kimi Räikkönen',\n",
        "                  'Charles Leclerc', 'Lewis Hamilton', 'Yuki Tsunoda',\n",
        "                  'Max Verstappen', 'Pierre Gasly', 'Fernando Alonso',\n",
        "                  'Sergio Pérez', 'Esteban Ocon', 'Antonio Giovinazzi',\n",
        "                  'Nikita Mazepin','Nicholas Latifi']\n",
        "data['active_driver'] = data['driver'].apply(lambda x: int(x in active_drivers))\n",
        "data['active_constructor'] = data['constructor'].apply(lambda x: int(x in active_constructors))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PSWfyzco6NU"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYGD1EC4o6NV"
      },
      "outputs": [],
      "source": [
        "data.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HFJvRoCo6NW"
      },
      "source": [
        "## Directory to store Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVm5JoLQo6NX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "if not os.path.exists('./models'):\n",
        "    os.mkdir('./models')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8l4HsK7eo6NY"
      },
      "outputs": [],
      "source": [
        "def position_index(x):\n",
        "    if x<4:\n",
        "        return 1\n",
        "    if x>10:\n",
        "        return 3\n",
        "    else :\n",
        "        return 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2v_NK8Uo6NZ"
      },
      "source": [
        "## Model considering only Drivers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHc3ciWro6Na"
      },
      "outputs": [],
      "source": [
        "x_d= data[['GP_name','quali_pos','driver','age_at_gp_in_days','position','driver_confidence','active_driver']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T3g0nmgQo6Na"
      },
      "outputs": [],
      "source": [
        "x_d = x_d[x_d['active_driver']==1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PpkliRgzo6Nb"
      },
      "outputs": [],
      "source": [
        "sc  = StandardScaler()\n",
        "le = LabelEncoder()\n",
        "x_d['GP_name'] = le.fit_transform(x_d['GP_name'])\n",
        "x_d['driver'] = le.fit_transform(x_d['driver'])\n",
        "x_d['GP_name'] = le.fit_transform(x_d['GP_name'])\n",
        "x_d['age_at_gp_in_days'] = sc.fit_transform(x_d[['age_at_gp_in_days']])\n",
        "X_d = x_d.drop(['position','active_driver'],1)\n",
        "y_d = x_d['position'].apply(lambda x: position_index(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2O4kwNlo6Nb"
      },
      "outputs": [],
      "source": [
        "#cross validation for diffrent models\n",
        "models = [LogisticRegression(),DecisionTreeClassifier(),RandomForestClassifier(),SVC(),GaussianNB(),KNeighborsClassifier(), GradientBoostingClassifier()]\n",
        "names = ['LogisticRegression','DecisionTreeClassifier','RandomForestClassifier','SVC','GaussianNB','KNeighborsClassifier', 'GradientBoostingClassifier']\n",
        "model_dict = dict(zip(models,names))\n",
        "mean_results_dri = []\n",
        "results_dri = []\n",
        "name = []\n",
        "for model in models:\n",
        "    cv = StratifiedKFold(n_splits=10,random_state=1,shuffle=True)\n",
        "    result = cross_val_score(model,X_d,y_d,cv=cv,scoring='accuracy')\n",
        "    mean_results_dri.append(result.mean())\n",
        "    results_dri.append(result)\n",
        "    name.append(model_dict[model])\n",
        "    print(f'{model_dict[model]} : {result.mean()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZeYizXEo6Nb"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15,10))\n",
        "plt.boxplot(x=results_dri,labels=name)\n",
        "plt.xlabel('Models')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Model performance comparision (drivers only)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwMbCGmBo6Nc"
      },
      "source": [
        "## Model considering only Constructors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_3I2Ho3o6Nc"
      },
      "outputs": [],
      "source": [
        "x_c = data[['GP_name','quali_pos','constructor','position','constructor_reliability','active_constructor']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sMCT1qmMo6Nc"
      },
      "outputs": [],
      "source": [
        "x_c = x_c[x_c['active_constructor']==1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFTUXjHeo6Nd"
      },
      "outputs": [],
      "source": [
        "sc  = StandardScaler()\n",
        "le = LabelEncoder()\n",
        "x_c['GP_name'] = le.fit_transform(x_c['GP_name'])\n",
        "x_c['constructor'] = le.fit_transform(x_c['constructor'])\n",
        "X_c = x_c.drop(['position','active_constructor'],1)\n",
        "y_c = x_c['position'].apply(lambda x: position_index(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zbNyHIso6Nd"
      },
      "outputs": [],
      "source": [
        "#cross validation for diffrent models\n",
        "models = [LogisticRegression(),DecisionTreeClassifier(),RandomForestClassifier(),SVC(),GaussianNB(),KNeighborsClassifier(), GradientBoostingClassifier()]\n",
        "names = ['LogisticRegression','DecisionTreeClassifier','RandomForestClassifier','SVC','GaussianNB','KNeighborsClassifier', 'GradientBoostingClassifier']\n",
        "model_dict = dict(zip(models,names))\n",
        "mean_results_const = []\n",
        "results_const = []\n",
        "name = []\n",
        "for model in models:\n",
        "    cv = StratifiedKFold(n_splits=10,random_state=1,shuffle=True)\n",
        "    result = cross_val_score(model,X_c,y_c,cv=cv,scoring='accuracy')\n",
        "    mean_results_const.append(result.mean())\n",
        "    results_const.append(result)\n",
        "    name.append(model_dict[model])\n",
        "    print(f'{model_dict[model]} : {result.mean()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GtSMYuoxo6Nf"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15,10))\n",
        "plt.boxplot(x=results_const,labels=name)\n",
        "plt.xlabel('Models')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Model performance comparision (Teams only)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HBCXcXQo6Ng"
      },
      "source": [
        "# Model considering both Drivers and Constructors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8Uky5n6o6Nh"
      },
      "outputs": [],
      "source": [
        "cleaned_data = data[['GP_name','quali_pos','constructor','driver','position','driver_confidence','constructor_reliability','active_driver','active_constructor']]\n",
        "cleaned_data = cleaned_data[(cleaned_data['active_driver']==1)&(cleaned_data['active_constructor']==1)]\n",
        "cleaned_data.to_csv('./data_f1/cleaned_data.csv',index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3k8IBx3o6Nh"
      },
      "source": [
        "### Build your X dataset with next columns:\n",
        "- GP_name\n",
        "- quali_pos to predict the classification cluster (1,2,3) \n",
        "- constructor\n",
        "- driver\n",
        "- position\n",
        "- driver confidence\n",
        "- constructor_reliability\n",
        "- active_driver\n",
        "- active_constructor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUc1osx1o6Ni"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_Xpavsko6Nj"
      },
      "source": [
        "### Filter the dataset for this Model \"Driver + Constructor\" all active drivers and constructors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-bFS1Duo6Nk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "ZyGxcJ43o6Nk"
      },
      "source": [
        "### Create Standard Scaler and Label Encoder for the different features in order to have a similar scale for all features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tUhDIX3Ko6Nl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQLIcbMio6Nl"
      },
      "source": [
        "### Prepare the X (Features dataset) and y for predicted value. \n",
        "In our case, we want to calculate the cluster of final position for ech driver using the \"position_index\" function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cr7F0brgo6Nl"
      },
      "outputs": [],
      "source": [
        "# Implement X, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCs9hgf3o6Nm"
      },
      "source": [
        "### Applied the same list of ML Algorithms for cross validation of different models\n",
        "\n",
        "And Store the accuracy Mean Value in order to compare with previous ML Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IStGVns-o6Nm"
      },
      "outputs": [],
      "source": [
        "mean_results = []\n",
        "results = []\n",
        "name = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dcl9r7DWo6Nn"
      },
      "outputs": [],
      "source": [
        "# cross validation for different models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AePGlOzo6No"
      },
      "source": [
        "### Use the same boxplot plotter used in the previous Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5x61mfZvo6No"
      },
      "outputs": [],
      "source": [
        "# Implement boxplot\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnQVkdVMo6No"
      },
      "source": [
        "# Comparing The 3 ML Models\n",
        "\n",
        "Let's see mean score of our three assumptions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUNqZtM2o6Np"
      },
      "outputs": [],
      "source": [
        "lr = [mean_results[0],mean_results_dri[0],mean_results_const[0]]\n",
        "dtc = [mean_results[1],mean_results_dri[1],mean_results_const[1]]\n",
        "rfc = [mean_results[2],mean_results_dri[2],mean_results_const[2]]\n",
        "svc = [mean_results[3],mean_results_dri[3],mean_results_const[3]]\n",
        "gnb = [mean_results[4],mean_results_dri[4],mean_results_const[4]]\n",
        "knn = [mean_results[5],mean_results_dri[5],mean_results_const[5]]\n",
        "gbc = [mean_results[6],mean_results_dri[6],mean_results_const[6]]\n",
        "font1 = {\n",
        "    'family':'serif',\n",
        "    'color':'black',\n",
        "    'weight':'normal',\n",
        "    'size':18\n",
        "}\n",
        "font2 = {\n",
        "    'family':'serif',\n",
        "    'color':'black',\n",
        "    'weight':'bold',\n",
        "    'size':12\n",
        "}\n",
        "x_ax = np.arange(3)\n",
        "plt.figure(figsize=(30,15))\n",
        "bar1 = plt.bar(x_ax,lr,width=0.1,align='center', label=\"Logistic Regression\")\n",
        "bar2 = plt.bar(x_ax+0.1,dtc,width=0.1,align='center', label=\"DecisionTree\")\n",
        "bar3 = plt.bar(x_ax+0.2,rfc,width=0.1,align='center',  label=\"RandomForest\")\n",
        "bar4 = plt.bar(x_ax+0.3,svc,width=0.1,align='center', label=\"SVC\")\n",
        "bar5 = plt.bar(x_ax+0.4,gnb,width=0.1,align='center', label=\"GaussianNB\")\n",
        "bar6 = plt.bar(x_ax+0.5,knn,width=0.1,align='center', label=\"KNN\")\n",
        "bar7 = plt.bar(x_ax+0.6, gbc, width=0.1, align='center', label=\"GBC\")\n",
        "plt.text(0.05,1,'CV score for combined data',fontdict=font1)\n",
        "plt.text(1.04,1,'CV score only driver data',fontdict=font1)\n",
        "plt.text(2,1,'CV score only team data',fontdict=font1)\n",
        "for bar in bar1.patches:\n",
        "    yval = bar.get_height()\n",
        "    plt.text(bar.get_x()+0.01,yval+0.01,f'{round(yval*100,2)}%',fontdict=font2)\n",
        "for bar in bar2.patches:\n",
        "    yval = bar.get_height()\n",
        "    plt.text(bar.get_x()+0.01,yval+0.01,f'{round(yval*100,2)}%',fontdict=font2)\n",
        "for bar in bar3.patches:\n",
        "    yval = bar.get_height()\n",
        "    plt.text(bar.get_x()+0.01,yval+0.01,f'{round(yval*100,2)}%',fontdict=font2)\n",
        "for bar in bar4.patches:\n",
        "    yval = bar.get_height()\n",
        "    plt.text(bar.get_x()+0.01,yval+0.01,f'{round(yval*100,2)}%',fontdict=font2)\n",
        "for bar in bar5.patches:\n",
        "    yval = bar.get_height()\n",
        "    plt.text(bar.get_x()+0.01,yval+0.01,f'{round(yval*100,2)}%',fontdict=font2)\n",
        "for bar in bar6.patches:\n",
        "    yval = bar.get_height()\n",
        "    plt.text(bar.get_x()+0.01,yval+0.01,f'{round(yval*100,2)}%',fontdict=font2)\n",
        "for bar in bar7.patches:\n",
        "    yval = bar.get_height()\n",
        "    plt.text(bar.get_x()+0.01,yval+0.01,f'{round(yval*100,2)}%',fontdict=font2)\n",
        "plt.legend(loc='center', bbox_to_anchor=(0.5, -0.10), shadow=False, ncol=6)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8W5w9FeZo6Np"
      },
      "outputs": [],
      "source": [
        "end = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OhSse3iqo6Nq"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "str(datetime.timedelta(seconds=(end - start)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TU4s6a1Zo6Nr"
      },
      "outputs": [],
      "source": [
        "print(str(end - start)+\" seconds\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}